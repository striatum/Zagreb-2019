{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1><center>\n",
    "    Artificial Neural Networks<br><br>\n",
    "     against Natural Language Ailments\n",
    "</center></h1>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<h3><center>\n",
    "    Petar Milin\n",
    "    <br>\n",
    "    University of Birmingham\n",
    "</center></h3>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "    Zagreb, Croatia\n",
    "    <br>\n",
    "    December 6, 2019\n",
    "</center>\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The History\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Beginning (40s)\n",
    "\n",
    "- 1943: McCulloch & Pitts hypothesize how neurons might work by relying on analogy to electrical circuits\n",
    "- 1949: Hebb stipulates the basic principle of association of neurons by means of neural co-activation (i.e., assembling)\n",
    "\n",
    "### First applications (60s)\n",
    "\n",
    "- 1958: Rosenblatt's work on the *Perceptron*\n",
    "- 1960: Widrow & Hoff propose the rule that was the first to be successfully applied to real-life problems (signal filtering)\n",
    "- 1960: Kalman independently proposes a very similar but more general principle for signal filtering (again sucessfully applied, this time in Apollo navigation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Silence\n",
    "\n",
    "- For about a decade, the development of neurologically inspired models stalled\n",
    "    + Theoretically - criticism about limited value for theory; culminated in Minsky and Papert's (1988) work\n",
    "    + Practically - machines implementing von Neumannâ€™s architecture were favoured\n",
    "\n",
    "### Re-inventions (70s & 80s)\n",
    "\n",
    "- 1972: Widrow-Hoff rule was (incidentaly and independently) re-branded\n",
    "    + **Psychology** - Rescorla and Wagner rule of learning\n",
    "    + **Computer Science** - Kohonen's Self-Organizing Maps\n",
    "\n",
    "\n",
    "- 1986: birth of **Connectionism** (a.k.a., Parallel Distributed Processing - PDP) by Rumelhart & McClelland\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Artificial Neuron\n",
    "\n",
    "<img src=\"./figs/neuron.jpg\" width=\"400\"/>\n",
    "\n",
    "- - -\n",
    "\n",
    "### Elements\n",
    "\n",
    "- $X_{i}$ is the $i$-th input unit with\n",
    "- $x_{i}$ input signal, which is weighted (or scaled) by the weight\n",
    "- $w_{i}$ to be transmitted to\n",
    "- $Y$ - the output unit, whose\n",
    "- $y_{in}$ is the net (or total) input and\n",
    "- $y_{out}$ is its activation (or output signal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Biological Neuron\n",
    "\n",
    "<img src=\"./figs/neuron_med.jpg\" width=\"600\"/>\n",
    "\n",
    "- - -\n",
    "\n",
    "### Elements\n",
    "\n",
    "- Dendrites $\\rightarrow X_{i}$\n",
    "- Cell body (i.e., Soma) $\\rightarrow Y$\n",
    "- Axon, which sends the output signal $\\rightarrow y_{out}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Signal and the Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Total Input Signal\n",
    "\n",
    "- Each input unit ($X_{i}$) sends a signal ($x_{i}$)\n",
    "- That signal is weighted ($w_{i}$) for how *valuable* it is against some criterion\n",
    "- By combining input signals and their weights we get the net input ($y_{in}$)\n",
    "\n",
    "\n",
    "- We first multiply each input signal ($x_{i}$) and its weight ($w_{i}$) to get the weighted input signal ($x_{i}w_{i}$)\n",
    "- Then we sum up all of them to get the total input signal ($y_{in}$)<br>\n",
    "\n",
    "$$ y_{in} = x_{1}w_{1} + x_{2}w_{2} + \\ldots + x_{n}w_{n} = \\sum_{i=1}^{n}{x_{i}w_{i}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Activation\n",
    "\n",
    "- The activation ($y_{out}$) is based on the total input signal ($y_{in}$)\n",
    "- It is said that it represents *a function* of the total input signal<br>\n",
    "\n",
    "$$y_{out} = f({y_{in}})$$\n",
    "\n",
    "\n",
    "- A very simple function would be to \"fire\" if the total input signal is greater than `0`<br>\n",
    "\n",
    "$$\n",
    "y_{out} = f({y_{in}}) =\n",
    "\\begin{cases}\n",
    "    1,& \\text{if $y_{in} > 0$} \\\\\n",
    "    0,& \\text{if $y_{in} \\leq 0$}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Activation\n",
    "\n",
    "- The activation ($y_{out}$) is based on the total input signal ($y_{in}$)\n",
    "- It is said that it represents *a function* of the total input signal<br>\n",
    "\n",
    "$$y_{out} = f({y_{in}})$$\n",
    "\n",
    "\n",
    "- A very simple function would be to \"fire\" if the total input signal is greated than `0`<br>\n",
    "\n",
    "$$\n",
    "y_{out} = f({y_{in}}) =\n",
    "\\begin{cases}\n",
    "    1,& \\text{if $y_{in} > 0$} \\\\\n",
    "    0,& \\text{if $y_{in} \\leq 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "- If we wish to complicate matters slightly, we can introduce \"polar\" (`+/-`) firing<br>\n",
    "\n",
    "$$\n",
    "y_{out} = f({y_{in}}) =\n",
    "\\begin{cases}\n",
    "    1,& \\text{if $y_{in} > 0$} \\\\\n",
    "    0,& \\text{if $y_{in} = 0$} \\\\\n",
    "    -1,& \\text{if $y_{in} < 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- This type of activation is/was often used for Peceptrons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Learning Rule\n",
    "\n",
    "<br>\n",
    "\n",
    "- Learning is gradual and assumes small changes in the weights that associate an input signal with the output\n",
    "\n",
    "\n",
    "- Why **small** changes?\n",
    "- Learning is gradual ...\n",
    "\n",
    "\n",
    "- This embodies the Principle of Minimal Disturbance\n",
    "    + \"\\[reduce\\] the output error for the current training pattern, with minimal disturbance to responses already learned\" (Widrow & Lehr, 1990, p. 1423)\n",
    "    + \"in the light of new input data, the parameters of an adaptive system should only be disturbed in a minimal fashion\" (Haykin, 2005, p. 436)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Perceptron Learning Rule\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$ \\Delta w_{i} = \\gamma \\times (t - y_{out}) \\times x_{i} $$\n",
    "\n",
    "- $\\gamma$ - learning rate, the one which ensures gradual learning (or causing minimal disturbance)\n",
    "- $t$ - **t**eacher or the outcome - *criterion* in learning\n",
    "- $y_{out}$ - output signal\n",
    "- $x_{i}$ - signal $x$ from the $i$-th input unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Perceptron Learning Rule\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$ \\Delta w_{i} = \\gamma \\times (t - y_{out}) \\times x_{i} $$\n",
    "\n",
    "- $\\gamma$ - learning rate, the one which ensures gradual learning (or causing minimal disturbance)\n",
    "- $t$ - **t**eacher or the outcome - *criterion* in learning\n",
    "- $y_{out}$ - output signal\n",
    "- $x_{i}$ - signal $x$ from the $i$-th input unit\n",
    "\n",
    "$$\n",
    "y_{out} = f({y_{in}}) =\n",
    "\\begin{cases}\n",
    "    1,& \\text{if $y_{in} > 0$} \\\\\n",
    "    0,& \\text{if $y_{in} = 0$} \\\\\n",
    "    -1,& \\text{if $y_{in} < 0$}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Widrow-Hoff / Delta / Least Mean Square (LMS) Rule\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$ \\Delta w_{i} = \\gamma \\times (t - y_{in}) \\times x_{i} $$\n",
    "\n",
    "\n",
    "- $\\gamma$ - learning rate, the one which ensures gradual learning (or causing minimal disturbance)\n",
    "- $t$ - **t**eacher or the outcome - *criterion* in learning\n",
    "- $y_{in}$ - total input signal\n",
    "- $x_{i}$ - signal $x$ from the $i$-th input unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Widrow-Hoff / Delta / Least Mean Square (LMS) Rule\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$$ \\Delta w_{i} = \\gamma \\times (t - y_{in}) \\times x_{i} $$\n",
    "\n",
    "\n",
    "- $\\gamma$ - learning rate, the one which ensures gradual learning (or causing minimal disturbance)\n",
    "- $t$ - **t**eacher or the outcome - *criterion* in learning\n",
    "- $y_{in}$ - total input signal\n",
    "- $x_{i}$ - signal $x$ from the $i$-th input unit\n",
    "\n",
    "<br>\n",
    "\n",
    "- Note that **WH** uses only the net input ($y_{in}$) and it does not even need the activation ($y_{out}$) un like the Perceptron \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "This is an example that you can (and should) try with your pocket calculator and paper and pencil, to fully understand how does the update of association weights happen. The example is presented in a pdf-file: `Handmade_Tomatoes.pdf`.\n",
    "\n",
    "[Handmade_Tomatoes.pdf](./Handmade_Tomatoes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "This is the same example like the one above, but this time you will be using Python to observe the same process of learning the weights. To run this code, please open the jupyter notebook `Python_Tomatoes.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "The final example is linguistically more meaningful and it uses verbs **start** and **begin** to test whether the machine (or human) can learn when to use themfrom exposure to examples.\n",
    "\n",
    "This time, however, you need to do a bit more:\n",
    "\n",
    "- use the existing code that shows you how to start\n",
    "- then use the code from the Exercise 2 (`Python_Tomatoes.ipynb`) and copy and adjust whatever you think you would need to make the training.\n",
    "\n",
    "The beginning code is in jupyter notebook `Verb_Semantics.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (More) Complex Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dense networks\n",
    "\n",
    "<img src=\"./figs/adelines.jpg\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recurrent networks\n",
    "\n",
    "<img src=\"./figs/recurrent.png\" width=\"600\"/>\n",
    "\n",
    "Image: FranÃ§ois Deloche\n",
    "\n",
    "- - -\n",
    "\n",
    "- The most common type is the Long Short-Term Memory (LSTM) learning network\n",
    "\n",
    "\n",
    "- Recurrent layers are similar to dense networks, but they also feed their output back into themselves as input\n",
    "- And because they are connected to themselves they\n",
    "    + can have a certain sort of memory\n",
    "    + can learn temporal patterns\n",
    "\n",
    "\n",
    "- Natural language processing (NLP) systems and speech recognition (SR) systems both use this type of networks very commonly (cf., Graves & Schmidhuber, 2005; Graves et al., 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**THE END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
